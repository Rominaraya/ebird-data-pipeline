# üê¶ eBird Data Pipeline
[![Pipeline eBird](https://github.com/Rominaraya/ebird-data-pipeline/actions/workflows/pipeline_ebird.yml/badge.svg)](https://github.com/Rominaraya/ebird-data-pipeline/actions/workflows/pipeline_ebird.yml)
![Python](https://img.shields.io/badge/python-3.13.7-blue.svg)

Pipeline automatizado para la recolecci√≥n, transformaci√≥n y almacenamiento de datos de observaciones de aves de la localidad seleccionada, utilizando la API de eBird, Google Cloud Storage y BigQuery.  
La ejecuci√≥n est√° programada y gestionada mediante GitHub Actions.

## Uso de datos de eBird
Los datos utilizados en este proyecto provienen de la API p√∫blica de eBird.  
Fuente recomendada para citaci√≥n:  
eBird. 2025. eBird Basic Dataset. Cornell Lab of Ornithology, Ithaca, New York.

## Intenci√≥n del proyecto
El objetivo principal es crear un flujo de datos automatizado, reproducible y escalable que permita:

- Obtener registros actualizados por d√≠a de observaciones de aves de una localidad espec√≠fica desde la API de eBird.
- Procesar y limpiar los datos para asegurar consistencia y trazabilidad.
- Almacenarlos en Google Cloud Storage y BigQuery para an√°lisis posteriores.

---

## Funcionalidad

1. **Ingesta (`ingest.py`)**  
   - Consulta la API de eBird para obtener observaciones del d√≠a anterior de una localidad seleccionada.  
   - Genera archivos crudos y los sube a Google Cloud Storage en la carpeta `raw/`.

2. **Transformaci√≥n (`transform.py`)**  
   - Descarga los archivos desde `raw/`.  
   - Convierte los datos a formato tabular (CSV).  
   - A√±ade campos de control como `timestamp` (hora de observaci√≥n) y `execution_time` (hora de ejecuci√≥n del pipeline).  
   - Estandariza columnas y elimina duplicados.  
   - Guarda los resultados en la carpeta `processed/` y en GCS.

3. **Carga (`load.py`)**  
   - Toma el √∫ltimo archivo procesado.  
   - Inserta los datos en una tabla de BigQuery (`ebird_data.observations`).  

4. **Orquestaci√≥n (`run_pipeline.py`)**  
   - Ejecuta en orden los pasos de ingesta, transformaci√≥n y carga.  
   - Incluye logging y control de errores para trazabilidad.

5. **Automatizaci√≥n (`.github/workflows/pipeline_ebird.yml`)**  
   - Ejecuci√≥n programada todos los d√≠as a las 12:00 hora de Chile (15:00 UTC).  
   - Ejecuci√≥n manual disponible mediante `workflow_dispatch`.  
   - Configuraci√≥n de entorno y autenticaci√≥n a GCP mediante secretos.

---

## Variables de entorno

El pipeline requiere las siguientes variables (configuradas como GitHub Secrets o en un archivo `.env` local):

- `EBIRD_API_KEY` ‚Üí API key de eBird.  
- `GCS_BUCKET_NAME` ‚Üí Nombre del bucket en Google Cloud Storage.  
- `GCP_CREDENTIALS` ‚Üí Credenciales JSON de servicio para GCP.  

---
## Obtenci√≥n de la API Key de eBird

Para ejecutar este pipeline es necesario contar con una API Key personal de eBird.  
El proceso para obtenerla es el siguiente:

1. Crear una cuenta gratuita en [eBird](https://ebird.org) si a√∫n no la tienes.  
2. Iniciar sesi√≥n y dirigirse a la p√°gina de [solicitud de API Key](https://documenter.getpostman.com/view/664302/S1ENwy59).  
3. Generar la clave y copiarla.  
4. Configurarla como variable de entorno `EBIRD_API_KEY` en tu entorno local o en los Secrets de GitHub Actions.  

---

## Tecnolog√≠as utilizadas

- Python 3.13.7  
- eBird API  
- Google Cloud Storage  
- Google BigQuery  
- GitHub Actions (automatizaci√≥n)  
- Pandas (procesamiento de datos)

---

## Ejecuci√≥n local

```bash
# Crear entorno virtual
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
.venv\Scripts\activate      # Windows

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar pipeline
python src/run_pipeline.py

---